---
layout: ../../../layouts/BaseLayout.astro
title: Getting started
description: Initialiser k8s
---

# Introduction
Kubernetes, souvent abr√©g√© en K8s (le 8 repr√©sente les lettres entre le K et le s), est une plateforme open-source d‚Äôorchestration de conteneurs.
Elle permet de d√©ployer, mettre √† l‚Äô√©chelle, et g√©rer automatiquement des applications conteneuris√©es dans un environnement distribu√©. Aujourd'hui K8s se pr√©sente comme l'un 
des standards de l‚Äôorchestration de conteneurs.

Cr√©√© par Google en 2014 (issu de leur exp√©rience interne avec Borg), Kubernetes est maintenant maintenu par la Cloud Native Computing Foundation (CNCF).
Plus de 75 % des projets de la CNCF sont √©crits en Go. Kubernetes, Terraform, Docker, Helm, Istio, et de nombreux autres outils de la CNCF sont d√©velopp√©s en Go.

---

### üéØ Objectifs principaux de Kubernetes
Kubernetes a pour mission de :

- Automatiser le d√©ploiement et le red√©marrage des applications

- G√©rer la mont√©e en charge (scalabilit√© automatique)

- Isoler, organiser, et superviser des groupes de conteneurs

- Fournir des services r√©seaux internes et externes

- G√©rer la configuration et les secrets

- Orchestrer des mises √† jour progressives (rolling updates)


---
#### Pourquoi utiliser Kubernetes ?
Tant que votre application conteneuris√©e est d√©ploy√©e sur **une seule machine**, et si cela suffit niveau disponibilit√©, il **n‚Äôest pas n√©cessaire d‚Äôutiliser Kubernetes**. Cependant, K8s devient indispensable dans les cas suivants :

- Red√©marrage manuel d‚Äôun service en cas de crash

- Scripts maison pour g√©rer le d√©ploiement

- Scalabilit√© difficile

- Difficult√© √† g√©rer l‚Äô√©tat d‚Äôune application distribu√©e

- R√©seau et s√©curit√© mal int√©gr√©s

---

#### Ce que Kubernetes n‚Äôest pas

- Un syst√®me de build d‚Äôimages (‚Üí utilise Docker, Buildah, etc.)

- Un orchestrateur de machines virtuelles (c‚Äôest pour les conteneurs)

- Une solution de CI/CD (mais il s‚Äôint√®gre tr√®s bien avec des outils comme GitLab CI, ArgoCD, etc.)

---
# Pr√©requis

Kubernetes (K8s) s‚Äôappuie fortement sur des notions de **r√©seau**. Au fil de votre apprentissage vous serez surement confront√© aux notions ci dessous :
- L‚Äôadressage IP (r√©seaux priv√©s/publics)
- Les ports, protocoles TCP/UDP
- Le fonctionnement du DNS interne
- Le routage et le load balancing
- Une familiarit√© avec `iptables` (pare-feu Linux) est un plus

---

### üê≥ Docker comme base

**Docker** peut √™tre per√ßu comme une **technologie introductive**.  
Kubernetes r√©utilise les **concepts de conteneurs**, mais en les **√©tendant √† l‚Äô√©chelle du cluster** :
- Orchestration
- R√©partition de charge
- R√©silience et auto-r√©paration

---

### üì¶ Distributions Kubernetes

Il existe plusieurs **distributions** de Kubernetes, certaines propos√©es par des **fournisseurs cloud priv√©s**, d'autres **open source** plus l√©g√®res adapt√©es √† l‚Äôapprentissage ou √† l‚Äôedge computing, comme **K3s**.

Voici quelques exemples de distributions courantes :

| Distribution | Particularit√©                                                  |
|--------------|---------------------------------------------------------------|
| **GKE (Google)** | Version manag√©e par Google Cloud, payante                  |
| **EKS (AWS)**    | Version manag√©e sur Amazon                                 |
| **AKS (Azure)**  | Version manag√©e sur Microsoft Azure                        |
| **K3s**          | Distribution l√©g√®re, id√©ale pour edge computing ou dev     |
| **MicroK8s**     | Version l√©g√®re propos√©e par Canonical (Ubuntu)            |
| **Minikube**     | D√©ploiement local pour apprentissage ou tests             |
| **OpenShift**    | Distribution Red Hat, avec s√©curit√© renforc√©e et CI/CD int√©gr√©e |

---

### üêß Connaissances syst√®me Linux

La gestion des **Pods** et de l‚Äôenvironnement Kubernetes n√©cessite des comp√©tences en **administration syst√®me Linux**.  
Ces comp√©tences sont utiles pour :
- **Configurer** les diff√©rents n≈ìuds (serveurs/agents)
- **D√©bugger** des erreurs syst√®me ou r√©seau
- **Surveiller** les processus et services critiques

Comp√©tences recommand√©es :
- Utilisation de la ligne de commande (bash, `ssh`, `scp`)
- Gestion des services (`systemctl`, `journalctl`)
- Manipulation de fichiers (`vim`, `nano`, `cp`, `mv`)
- Surveillance syst√®me (`ps`, `top`, `htop`, `kill`)

---

# Architecture

## Serveurs et agents

Un n≈ìud de serveur est d√©fini comme un h√¥te ex√©cutant la k3s **servercommande**, avec des composants de plan de contr√¥le et de banque de donn√©es g√©r√©s par K3s.
Un n≈ìud d'agent est d√©fini comme un h√¥te ex√©cutant la k3s **agentcommande**, sans aucun composant de banque de donn√©es ou de plan de contr√¥le.
Les serveurs et les agents ex√©cutent le kubelet, l'environnement d'ex√©cution du conteneur et le CNI. Consultez la documentation sur les options avanc√©es pour plus d'informations sur l'ex√©cution de serveurs sans agent.

---

![Illustration entre serveur et agent](/thotitfy/k8s_&_devops/how-it-works-k3s-revised.png)

## Cas concret de la docucmentation
Les **n≈ìuds d'agent** sont enregistr√©s avec une **connexion WebSocket** initi√©e par le **k3s agentprocessus**, et cette connexion est 
maintenue par un √©quilibreur de charge c√¥t√© client, ex√©cut√© dans le cadre du processus d'agent. 

Initialement, **l'agent se connecte au superviseur** (et √† **kube-apiserver**) via l'√©quilibreur de charge local sur le port 6443.
**L'√©quilibreur de charge g√®re une liste de points de terminaison disponibles**. 

Le **point de terminaison** par d√©faut (initialement unique) est initialis√© par **le nom d'h√¥te de l' --serveradresse**. Une fois connect√© au cluster,
**l'agent r√©cup√®re une liste d'adresses kube-apiserver** √† partir de la **liste des points de terminaison du service Kubernetes dans l'espace de noms par d√©faut**.
Ces points de terminaison sont ajout√©s √† **l'√©quilibreur de charge**, qui **maintient** ensuite des **connexions stables**
√† tous les serveurs du cluster, fournissant ainsi une **connexion √† kube-apiserver qui tol√®re les pannes de serveurs individuels.**

Les **agents s'enregistreront** aupr√®s du serveur **√† l'aide du secret du cluster** de n≈ìuds, **ainsi que d'un mot de passe g√©n√©r√© al√©atoirement** pour le n≈ìud, stock√© dans
/etc/rancher/**node/password**. Le serveur stockera **les mots de passe des n≈ìuds individuels sous forme de secrets Kubernetes**,
et toute tentative ult√©rieure **devra utiliser le m√™me mot de passe**. Les secrets des mots de passe des n≈ìuds sont stock√©s dans l' kube-systemespace de noms, avec des noms utilisant le mod√®le **<host>.node-password.k3s**. Ceci permet de prot√©ger l'int√©grit√© des identifiants de n≈ìuds.

Si le /etc/rancher/node_r√©pertoire d'un agent est supprim√© ou si vous souhaitez rejoindre un n≈ìud sous un nom existant, 
**supprimez-le du cluster**. Cela effacera l'ancienne entr√©e du n≈ìud et son mot de passe secret, et permettra au n≈ìud de rejoindre le cluster.

*Si vous r√©utilisez fr√©quemment des noms d'h√¥tes, mais que vous ne parvenez pas √† supprimer les mots de passe secrets des n≈ìuds,
un identifiant de n≈ìud unique peut √™tre automatiquement ajout√© au nom d'h√¥te en lan√ßant les serveurs ou agents K3s √† l'aide de 
l' --with-node-idoption. Lorsque cette option est activ√©e, l'identifiant de n≈ìud est √©galement stock√© dans /etc/rancher/node/.*


---
## Control Panel 
Lorsque vous utilisez kubectl pour g√©rer le cluster, en back-end, vous communiquez avec le serveur d'API via des **API REST HTTP**. 
Cependant, **les composants internes du cluster**, comme le planificateur, le contr√¥leur, etc., communiquent avec le serveur d'API via **gRPC** .

La **communication entre le serveur API** et les **autres composants** du cluster s'effectue **via TLS** pour **emp√™cher tout acc√®s non autoris√© au cluster**.

---
## Etcd
Kubernetes est un syst√®me distribu√© qui n√©cessite **une base de donn√©es distribu√©e performante, telle qu‚Äôetcd**, pour fonctionner efficacement. 
etcd prend en charge la nature distribu√©e de Kubernetes en **agissant** √† la fois **comme un service de d√©couverte** (back-end) 
et **comme une base de donn√©es cl√©-valeur**. On peut la consid√©rer comme **le cerveau du cluster Kubernetes**.

---
## Kubelet (agent)
**Mini orchestrateur** qui ne g√®re pas le runtime.

Kubelet est un composant agent qui **s'ex√©cute sur chaque n≈ìud du cluster**. Il ne s'ex√©cute pas en tant que conteneur mais plut√¥t en tant que **d√©mon**, g√©r√© par **systemd**.

Voici quelques-uns des √©l√©ments cl√©s de kubelet.

Kubelet utilise l'interface **gRPC CRI (container runtime interface)** pour communiquer avec le runtime du conteneur.
Il expose √©galement un point de terminaison HTTP pour diffuser des journaux et fournit des sessions d'ex√©cution pour les clients.
Utilise l'interface de stockage de conteneur **(CSI) gRPC pour configurer les volumes de blocs.**
Il utilise le **plugin CNI** configur√© dans le cluster pour **allouer l'adresse IP du pod** et configurer les routes r√©seau et **les r√®gles de pare-feu n√©cessaires** pour le pod.

---
## Containerd

containerd **est un runtime de conteneurs** d√©velopp√© par Docker qui **g√®re le cycle de vie d‚Äôun conteneu**r sur une machine physique ou virtuelle (c‚Äôest-√†-dire un h√¥te).
**Il cr√©e, lance, arr√™te et supprime les conteneurs**. Il peut √©galement **extraire des images** de conteneurs des registres, **monter le stockage** et **activer la mise en r√©seau d‚Äôun conteneur**.

En f√©vrier **2019, containerd est devenu un projet officiel** au sein de la **Cloud Native Computing Foundation (CNCF)**, au m√™me titre que Kubernetes, Prometheus, Envoy et CoreDNS.

---
## Proxy Kube 

Kube-proxy est un d√©mon qui s'ex√©cute sur chaque n≈ìud en tant que daemonset . Ce composant proxy impl√©mente le concept de services Kubernetes pour les pods (DNS unique pour un ensemble de pods avec √©quilibrage de charge). Il utilise principalement les proxys UDP, TCP et SCTP et ne comprend pas HTTP.

Lorsque vous exposez des pods √† l'aide d'un service (ClusterIP), Kube-proxy cr√©e des r√®gles r√©seau pour envoyer le trafic vers les pods backend (points de terminaison) regroup√©s sous l'objet Service. Autrement dit, l'√©quilibrage de charge et la d√©couverte de services sont g√©r√©s par le proxy Kube.

Vous ne pouvez pas envoyer de ping au ClusterIP car il est uniquement utilis√© pour la d√©couverte de services, contrairement aux IP de pod qui sont pingables.

---


# Concepts & Composants

## Namespaces

Les **namespaces** permettent d‚Äôisoler les ressources dans un cluster K8s.

- Sans namespace explicite, les ressources vont dans le `default`.
- Commande : `kubectl get all` ne montre que les ressources du namespace courant.
- Utilise `kubectl get all --all-namespaces` pour tout afficher.

---

## Pods

- Un **pod** est l‚Äôunit√© de base de Kubernetes.
- Contient **un ou plusieurs conteneurs** partageant IP, r√©seau et **stockage**.
- G√©r√© automatiquement : s‚Äôil crash, il est relanc√©.
- Cr√©√© par des objets comme **Deployment**, **DaemonSet**, etc.
---

## ClusterIP

**ClusterIP** est le type par **d√©faut** des **Services**. **Il expose un Pod √† l‚Äôint√©rieur du cluster uniquement** (pas √† l‚Äôext√©rieur).
Tr√®s utilis√© pour la **communication interservices**, m√™me si on ne le voit pas toujours directement.

---
## Objets principaux

| Objet         | R√¥le                                                                 |
|---------------|----------------------------------------------------------------------|
| **Pod**       | Unit√© d'ex√©cution contenant un ou plusieurs conteneurs.              |
| **Deployment**| Cr√©e et g√®re des pods r√©plicables (ex: frontend, API, workers), permet de configurer les replicaSet (on les configure tjrs les replica, jamais les replicaSet).      |
| **DaemonSet** | 1 pod par n≈ìud (ex: monitoring, agent de log).                       |
| **CronJob**   | Lance un pod √† intervalle r√©gulier (t√¢ches planifi√©es).              |
| **StatefulSet**| G√®re des pods avec identit√© stable (ex: base de donn√©es).           |
| **Service**   | Point d‚Äôacc√®s stable vers un ou plusieurs pods.                      |
| **Ingress**   | Reverse proxy pour exposer HTTP(S) avec routage par chemin/domaine.  |
| **Secret**    | Stocke des infos sensibles (mdp, tokens).                            |
| **ConfigMap** | Stocke des configs non sensibles (variables, fichiers).              |


## Types de Services

| Type de service  | Accessible depuis           | Description rapide                                                                 |
|------------------|-----------------------------|-------------------------------------------------------------------------------------|
| `ClusterIP`      | *Interne au cluster*        | Par d√©faut. Utilis√© pour les communications internes entre pods.                   |
| `NodePort`       | *IP de chaque n≈ìud*         | Ouvre un port statique (`<IP-node>:port`). Simplicit√© mais peu flexible.          |
| `LoadBalancer`   | *Internet (Cloud Provider)* | Cr√©e un vrai Load Balancer (cloud uniquement ou MetalLB en local).                |
| `ExternalName`   | *Redirection DNS*           | Redirige vers un DNS externe (ex: `api.example.com`).                              |
| **Ingress**      | *HTTP(S) unique*            | Reverse proxy pour exposer plusieurs services via un seul point d‚Äôentr√©e.         |


## ConfigMap et Secret

**ConfigMap, pour des donn√©es non sensibles.**
**Secret**, utilis√©e pour des **donn√©es sensibles**, comme des mots de passe, des cl√©s API ou des certificats. **Les donn√©es sont encod√©es en base64, ce qui n'est pas un v√©ritable chiffrement.**
Elles ne sont donc pas r√©ellement s√©curis√©es au repos (une fois stock√©es), mais c‚Äôest une **mesure minimale de protection.**

Pour aller plus loin il est possible de configurer k8s pour un r√©el chiffrement exemple: 

    apiVersion: apiserver.config.k8s.io/v1
    kind: EncryptionConfiguration
    resources:
      - resources:
          - secrets
        providers:
          - aescbc:
              keys:
                - name: key1
                  secret: <cl√© AES en base64>
          - identity: {}
  
Ici on utilise **aescbc** (chiffrement AES) dans les √©tapes de stockage des cl√©. K8s va d'abord passer par ce provider, en utilisant une cl√© g√©n√©r√©e
au pr√©alable pour chiffrer notre secret. (cf voir cours sur chiffrement)

## LoadBalancer

Un Service de type LoadBalancer est un objet Kubernetes qui **permet d‚Äôexposer un service du cluster √† l‚Äôext√©rieur**, 
en lui attribuant une **adresse IP externe (publique ou priv√©e)**. Cette adresse est g√©n√©ralement fournie par un **cloud provider** 
ou par **une solution interne** comme MetalLB dans un environnement on-premise (sur site).
Ce type de service agit comme une **passerelle r√©seau publique** : il 
**redirige le trafic entrant vers les Pods associ√©s au service √† l‚Äôint√©rieur du cluster**. Il n‚Äôest pas responsable de planifier les Pods (ce r√¥le revient au **Scheduler**), 
ni de faire du routage applicatif complexe (ce r√¥le revient souvent √† un **Ingress Controller**).

Ainsi le LoadBalancer permet √† des utilisateurs ou des syst√®mes ext√©rieurs au cluster d‚Äôacc√©der √† un service 
sp√©cifique, **sans intervenir dans la gestion des Pods ni dans la cr√©ation de r√©plicas**.

üìù *En local, utilise [MetalLB](https://metallb.universe.tf/) pour simuler ce comportement.*

**Exemple :** 

√Ä travers cette configuration, le LoadBalancer va associer une **IP publique au port 80** et **rediriger le trafic** vers 
**l‚Äôapplication interne nomm√©e my-app**, qui √©coute sur le **port 3000** dans les pods cibl√©s.

      apiVersion: v1
      kind: Service
      metadata:
        name: my-app-service
      spec:
        type: LoadBalancer
        selector:
          app: my-app
        ports:
          - port: 80           # Port externe
            targetPort: 3000 

---

### LoadBalancer & Garbage collector 
Dans le cas g√©n√©ral, les ressources de **load balancer** associ√©es chez le **cloud provider** devraient √™tre automatiquement supprim√©es peu de temps
apr√®s la suppression d‚Äôun Service de type LoadBalancer. Cependant, il est connu que divers cas limites (corner cases) peuvent entra√Æner l‚Äôapparition de **ressources orphelines**
(non supprim√©es) apr√®s la suppression du Service associ√©.

Pour √©viter cela, une protection par **Finalizer** a √©t√© introduite pour les Services LoadBalancer. 
Gr√¢ce √† l‚Äôutilisation des **finalizers, une ressource Service ne pourra jamais √™tre supprim√©e tant que les ressources load balancer correspondantes n‚Äôont pas √©t√© supprim√©es √©galement.**

Concr√®tement, si un Service est de type LoadBalancer, le service controller lui attache automatiquement un finalizer nomm√© :
    
    service.kubernetes.io/load-balancer-cleanup.

Ce **finalizer** ne sera supprim√© qu‚Äô**apr√®s le nettoyage effectif de la ressource de load balancer**. Cela permet d‚Äô√©viter les ressources orphelines, m√™me dans des corner cases comme un crash du service controller.

---
## Ingress (et pourquoi le pr√©f√©rer)

![Illustration ingress](/thotify/k8s_&_devops/ingress_archi.png)

### Ingress
Rendez votre service **r√©seau HTTP** (ou HTTPS) accessible en utilisant un m√©canisme de configuration "protocol-aware", qui comprend les concepts web comme les **URI**, **hostnames**, **paths**, etc.
Le concept d‚ÄôIngress vous permet de mapper le trafic vers diff√©rents backends selon des r√®gles que vous d√©finissez via l‚ÄôAPI Kubernetes.

**Un Ingress expose des routes HTTP et HTTPS depuis l‚Äôext√©rieur du cluster vers des services √† l‚Äôint√©rieur du cluster.**
Le routage du trafic est contr√¥l√© par des r√®gles d√©finies dans la ressource Ingress.

Un Ingress peut √™tre configur√© pour fournir aux Services des URLs accessibles de l‚Äôext√©rieur, faire du load balancing du trafic, g√©rer la termination SSL / TLS, et offrir un virtual hosting bas√© sur le nom de domaine.

Un **Ingress controller** est **responsable de la mise en ≈ìuvre de l‚ÄôIngress**, g√©n√©ralement √† l‚Äôaide d‚Äôun load balancer, mais il peut aussi configurer votre **edge router** (mini routeur physique) ou d‚Äôautres frontends pour **aider √† g√©rer le trafic**.

**Un Ingress n‚Äôexpose pas de ports ou de protocoles arbitraires.**
Pour **exposer des services autres que HTTP et HTTPS** sur Internet, on utilise typiquement un Service de type **NodePort** ou **LoadBalancer**.

#### Exemple ingress ressource
```yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: minimal-ingress
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: / # routage externe:interne pour le ingressController
    spec:
      ingressClassName: nginx-example # liens issu du nom du ingressController 
      rules:
      - http: # les requetes http pointant vers /testpath 
          paths: # seront redirig√© vers le service se nommant name:80
          - path: /testpath
            pathType: Prefix
            backend:
              service:
                name: test
                port:
                  number: 80
```

![Illustration du routage](/thotify/k8s_&_devops/ingress_routing.png)


### Ingress Controller

Pour qu‚Äôun Ingress fonctionne dans votre cluster, un ingress controller doit √™tre en cours d‚Äôex√©cution.
Vous devez s√©lectionner au moins un ingress controller (environ ~31 controller officiels) et vous assurer qu‚Äôil est correctement configur√© dans votre cluster.

| Ingress Controller | Remarques                               |
| ------------------ | --------------------------------------- |
| **NGINX**          | Le plus courant, stable, tr√®s utilis√©   |
| **Traefik**        | Moderne, l√©ger, dynamique               |
| **HAProxy**        | Performant, tr√®s configurable           |
| **Contour**        | Bas√© sur Envoy                          |
| **Istio Gateway**  | Si tu utilises Istio comme service mesh |
| **AWS ALB**        | Int√©gr√© dans l'infrastructure AWS       |
| **GKE Ingress**    | Fournisseur Google Cloud                |
|      ...           |                 .......                 |

Ingress = Objet Kubernetes
Ingress Controller = Pod qui impl√©mente cet objet (ex : nginx, traefik)

| Option        | ‚úÖ Avantages                          | ‚ùå Inconv√©nients                                     |
|---------------|--------------------------------------|-----------------------------------------------------|
| NodePort      | Facile pour tester en local          | Moins s√©curis√©, peu flexible                        |
| LoadBalancer  | Propre en cloud                      | Cher (un LB par service), inutile sans cloud        |
| Ingress       | Id√©al pour exposer plusieurs services | N√©cessite un Ingress Controller (NGINX, Traefik...) |

### Ingress = Le choix moderne

- Un seul point d‚Äôentr√©e
- Routage par chemin (`/api`, `/ws`) ou domaine (`api.monsite.com`)
- Support du HTTPS, Let‚Äôs Encrypt, cert-manager
- Scalable, maintenable, propre

---

## ConfigMap / Secret

| Ressource    | Utilit√©                             |
|--------------|--------------------------------------|
| **ConfigMap**| Param√®tres non sensibles (env, fichiers config) |
| **Secret**   | Donn√©es sensibles (tokens, mdp...)   |

exemples

**ConfigMap**
```yaml 
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: example-config
    data:
      # Donn√©es non sensibles sous forme cl√©/valeur
      database_host: "db.example.com"
      database_port: "5432"
      feature_flag: "true"
```
**Secret**
```yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: example-secret
    type: Opaque
    data:
      # Donn√©es sensibles encod√©es en base64
      username: YWRtaW4=        # "admin" encod√© en base64
      password: c2VjcmV0MTIz    # "secret123" encod√© en base64
```
**impl√©mentation Pod**
```yaml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: example-pod
    spec:
      containers:
        - name: my-container
          image: nginx
          env:
            - name: DB_HOST
              valueFrom:
                configMapKeyRef:
                  name: example-config
                  key: database_host
            - name: DB_USER
              valueFrom:
                secretKeyRef:
                  name: example-secret
                  key: username
```
## Stockage : Volume / PVC

- Un pod est **√©ph√©m√®re**, donc ses donn√©es disparaissent √† sa mort.
- Pour persister les donn√©es :
  - Utilise un `PersistentVolume`
  - Requis : `PersistentVolumeClaim` (PVC) pour s‚Äôy connecter
- Utilis√© avec `StatefulSet` ou pour les bases de donn√©es.

Un pod peut se configurer avec un PVC pour obtenir une persistance de don√©es. 
Par exemple, ici je cr√©er un PVC 

```yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: my-pvc
    spec:
      accessModes:
        - ReadWriteOnce           
      resources:
        requests:
          storage: 1Gi            # Taille demand√©e
      storageClassName: standard  # Classe de stockage (selon ton cluster)
```
Que je nomme **my-pvc**

et que j'associe a mon pod dans **volumes:**

```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-pod
    spec:
      containers:
      - name: my-container
        image: nginx
        volumeMounts:
        - name: my-storage
          mountPath: /usr/share/nginx/html   # chemin dans le conteneur
      volumes:
      - name: my-storage
        persistentVolumeClaim:
          claimName: my-pvc                 # r√©f√©rence au PVC
```
*Le volume peut √™tre mont√© en lecture-√©criture par un seul noeud √† la fois (sinon aller voir du cot√© de ReadWriteMany)
*
## Probes

- **Liveness Probe** : red√©marre le conteneur s‚Äôil ne r√©pond plus.
- **Readiness Probe** : signale si le conteneur est pr√™t √† recevoir du trafic.
- Important pour la sant√© et la stabilit√© des services.

## Les Quorums
(raft)

![Illustration du conscensus raft](/thotify/k8s_&_devops/raft_conscensus.gif)

K8s utilise etcd comme base de donn√©es centrale.
Etcd fonctionne √† quorum :

Si tu as 3 n≈ìuds etcd ‚Üí quorum = 2

Si tu as 5 n≈ìuds etcd ‚Üí quorum = 3

Et ainsi de suite : quorum = (N / 2) + 1

‚û°Ô∏è Si le quorum n‚Äôest pas atteint, plus aucune √©criture ne peut avoir lieu.

| Cas                       | Quorum utile pour...                                       |
| ------------------------- | ---------------------------------------------------------- |
| Kubernetes (etcd)         | √âviter corruption de l‚Äô√©tat du cluster                     |
| Kafka                     | G√©rer qui est le leader, assurer la coh√©rence des messages |
| Base de donn√©es r√©pliqu√©e | Assurer que la majorit√© a bien re√ßu les √©critures          |
| √âlection de leader        | D√©cider qui prend le contr√¥le sans conflit                 |

## RBAC Authorization

L'autorisation RBAC utilise lerbac.authorization.k8s.io Groupe APIpour piloter les d√©cisions d'autorisation, vous permettant de configurer dynamiquement les politiques via l'API Kubernetes.

Pour activer RBAC, d√©marrez leserveur API avec l' --authorization-configindicateur d√©fini sur un fichier qui inclut l' RBACautorisateur ; par exemple :

manifest example: 

  apiVersion: apiserver.config.k8s.io/v1
  kind: AuthorizationConfiguration
  authorizers:
    ...
    - type: RBAC
    ...

L'API RBAC d√©clare quatre types d'objets Kubernetes : Role , ClusterRole , RoleBinding et ClusterRoleBinding . Vous pouvez d√©crire ou modifier le RBAC. objets en utilisant des outils tels que kubectl, comme tout autre objet Kubernetes.

[Voir doc pour en apprendre plus sur: Role, ClusterRole, RoleBinding et ClusterRoleBinding](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)

## Other orchestrator 

### Docker Swarm

### Nomad 

[guide](https://blog.cloudflare.com/how-we-use-hashicorp-nomad/?ref=devopscube.com/)
![Illustration](/thotify/k8s_&_devops/image3.png)


## Tools

## For Dev Environment

1. Vagrant  
2. Docker Desktop  
3. Minikube (k8s)  
4. Minishift (k8s)  
5. Kind (k8s)  

## For Infrastructure Provisioning

1. Terraform (preferable)  
2. CloudFormation for AWS  
3. CLIs (of respective cloud provider)  
4. Pulumi  

## For Configuration Management

1. Ansible (preferable)  
2. Chef  
3. Puppet  
4. Saltstack  

## VM Image / Container Management

1. HashiCorp Packer  
2. Docker  


### Helm

Helm est un gestionnaire de configuration et de paquets pour Kubernetes.
Il permet de d√©ployer facilement des applications complexes sur un cluster Kubernetes √† l‚Äôaide de Helm Charts (graphes de d√©ploiement).

Il offre de puissantes fonctionnalit√©s de templating, prenant en charge la g√©n√©ration de templates pour tous les objets Kubernetes :
Deployments, Pods, Services, ConfigMaps, Secrets, RBAC, PSP, etc.

### Kustomize 
Kustomize repose sur deux concepts cl√©s : Base et Overlays.

Avec Kustomize, on peut r√©utiliser des fichiers de base (fichiers YAML communs) pour tous les environnements, et y appliquer des overlays (patchs) sp√©cifiques √† chaque environnement.

Le processus de overlaying consiste √† cr√©er une version personnalis√©e d‚Äôun manifest, en combinant :

manifest de base + manifest overlay = manifest personnalis√©

Toutes les sp√©cifications de personnalisation sont d√©finies dans un fichier appel√© kustomization.yaml.
![Illustration](/thotify/k8s_&_devops/image-29-22.png)

## Terminology 

- Node: A worker machine in Kubernetes, part of a cluster.
- Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.
- Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.
- Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes networking model.
- Service: A Kubernetes Service that identifies a set of Pods using label selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.























# Ressources
[Comprendre K8s dans le d√©tails, la plupart des liens ci-dessous sont tir√©s de cet article](https://devopscube.com/kubernetes-architecture-explained/)
[Comprendre la concurrence RAFT](https://raft.github.io/) 
[Th√©or√®me CAP & concurrence ](https://en.wikipedia.org/wiki/CAP_theorem?ref=devopscube.com)
[Gestion de la planification avanc√©e](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md)