---
layout: ../../../layouts/BaseLayout.astro
title: Getting started
description: Initialiser k8s
---

# Introduction
Plus de 75 % des projets de la CNCF sont √©crits en Go. Kubernetes, Terraform, Docker, Helm, Istio, et de nombreux autres outils de la CNCF sont d√©velopp√©s en Go.

Kubernetes (K8s) est une plateforme open-source d√©velopp√©e par Google (et maintenant maintenue par la CNCF) qui permet de **d√©ployer, g√©rer et mettre √† l‚Äô√©chelle automatiquement** des conteneurs (comme Docker) de fa√ßon d√©clarative.

---

## Pourquoi Kubernetes ?

Tant que votre application conteneuris√©e est d√©ploy√©e sur **une seule machine**, et si cela suffit niveau disponibilit√©, il **n‚Äôest pas n√©cessaire d‚Äôutiliser Kubernetes**. Cependant, K8s devient indispensable dans les cas suivants :

- Besoin de haute disponibilit√©, scalabilit√© ou tol√©rance aux pannes
- R√©partition automatique des charges
- Gestion simplifi√©e des volumes et du r√©seau
- Auto-r√©cup√©ration des services tomb√©s
- D√©ploiement multi-environnement

Quelques chiffres int√©ressants :
- Massivement utilis√© par les entreprises tech
- Soutenu par une grande communaut√©
- L'un des standards de l‚Äôorchestration de conteneurs

---

# Architecture

## Serveurs et agents

Un n≈ìud de serveur est d√©fini comme un h√¥te ex√©cutant la k3s servercommande, avec des composants de plan de contr√¥le et de banque de donn√©es g√©r√©s par K3s.
Un n≈ìud d'agent est d√©fini comme un h√¥te ex√©cutant la k3s agentcommande, sans aucun composant de banque de donn√©es ou de plan de contr√¥le.
Les serveurs et les agents ex√©cutent le kubelet, l'environnement d'ex√©cution du conteneur et le CNI. Consultez la documentation sur les options avanc√©es pour plus d'informations sur l'ex√©cution de serveurs sans agent.

[Illustration entre serveur et agent](/thotitfy/k8s_&_devops/how-it-works-k3s-revised-9c025ef482404bca2e53a89a0ba7a3c5.svg)

## Cas concret de la doc 
Les n≈ìuds d'agent sont enregistr√©s avec une connexion WebSocket initi√©e par le k3s agentprocessus, et cette connexion est maintenue par un √©quilibreur de charge c√¥t√© client, ex√©cut√© dans le cadre du processus d'agent. Initialement, l'agent se connecte au superviseur (et √† kube-apiserver) via l'√©quilibreur de charge local sur le port 6443. L'√©quilibreur de charge g√®re une liste de points de terminaison disponibles. Le point de terminaison par d√©faut (initialement unique) est initialis√© par le nom d'h√¥te de l' --serveradresse. Une fois connect√© au cluster, l'agent r√©cup√®re une liste d'adresses kube-apiserver √† partir de la liste des points de terminaison du service Kubernetes dans l'espace de noms par d√©faut. Ces points de terminaison sont ajout√©s √† l'√©quilibreur de charge, qui maintient ensuite des connexions stables √† tous les serveurs du cluster, fournissant ainsi une connexion √† kube-apiserver qui tol√®re les pannes de serveurs individuels.

Les agents s'enregistreront aupr√®s du serveur √† l'aide du secret du cluster de n≈ìuds, ainsi que d'un mot de passe g√©n√©r√© al√©atoirement pour le n≈ìud, stock√© dans /etc/rancher/node/password. Le serveur stockera les mots de passe des n≈ìuds individuels sous forme de secrets Kubernetes, et toute tentative ult√©rieure devra utiliser le m√™me mot de passe. Les secrets des mots de passe des n≈ìuds sont stock√©s dans l' kube-systemespace de noms, avec des noms utilisant le mod√®le <host>.node-password.k3s. Ceci permet de prot√©ger l'int√©grit√© des identifiants de n≈ìuds.

Si le /etc/rancher/noder√©pertoire d'un agent est supprim√© ou si vous souhaitez rejoindre un n≈ìud sous un nom existant, supprimez-le du cluster. Cela effacera l'ancienne entr√©e du n≈ìud et son mot de passe secret, et permettra au n≈ìud de rejoindre le cluster.

Si vous r√©utilisez fr√©quemment des noms d'h√¥tes, mais que vous ne parvenez pas √† supprimer les mots de passe secrets des n≈ìuds, un identifiant de n≈ìud unique peut √™tre automatiquement ajout√© au nom d'h√¥te en lan√ßant les serveurs ou agents K3s √† l'aide de l' --with-node-idoption. Lorsque cette option est activ√©e, l'identifiant de n≈ìud est √©galement stock√© dans /etc/rancher/node/.

## Control Panel 
Ainsi, lorsque vous utilisez kubectl pour g√©rer le cluster, en back-end, vous communiquez avec le serveur d'API via des API REST HTTP . Cependant, les composants internes du cluster, comme le planificateur, le contr√¥leur, etc., communiquent avec le serveur d'API via gRPC .

La communication entre le serveur API et les autres composants du cluster s'effectue via TLS pour emp√™cher tout acc√®s non autoris√© au cluster.


## Etcd

etcd
Kubernetes est un syst√®me distribu√© et n√©cessite une base de donn√©es distribu√©e performante, telle qu'etcd, qui prend en charge sa nature distribu√©e. Elle agit √† la fois comme un service back-end de d√©couverte et comme une base de donn√©es. On peut la qualifier de cerveau du cluster Kubernetes.

## Kubelet (agent)
Mini orchestrateur qui ne g√®re pas le runtime
Kubelet est un composant agent qui **s'ex√©cute sur chaque n≈ìud du cluster**. Il ne s'ex√©cute pas en tant que conteneur mais plut√¥t en tant que **d√©mon**, g√©r√© par **systemd**.

Voici quelques-uns des √©l√©ments cl√©s de kubelet.

Kubelet utilise l'interface **gRPC CRI (container runtime interface)** pour communiquer avec le runtime du conteneur.
Il expose √©galement un point de terminaison HTTP pour diffuser des journaux et fournit des sessions d'ex√©cution pour les clients.
Utilise l'interface de stockage de conteneur (CSI) gRPC pour configurer les volumes de blocs.
Il utilise le **plugin CNI** configur√© dans le cluster pour **allouer l'adresse IP du pod** et configurer les routes r√©seau et **les r√®gles de pare-feu n√©cessaires** pour le pod.

## Containerd

Qu‚Äôest-ce que containerd ?
containerd est un runtime de conteneurs d√©velopp√© par Docker qui g√®re le cycle de vie d‚Äôun conteneur sur une machine physique ou virtuelle (c‚Äôest-√†-dire un h√¥te). Il cr√©e, lance, arr√™te et supprime les conteneurs. Il peut √©galement extraire des images de conteneurs des registres, monter le stockage et activer la mise en r√©seau d‚Äôun conteneur.

En f√©vrier 2019, containerd est devenu un projet officiel au sein de la Cloud Native Computing Foundation (CNCF), au m√™me titre que Kubernetes, Prometheus, Envoy et CoreDNS.

## Proxy Kube 

Kube-proxy est un d√©mon qui s'ex√©cute sur chaque n≈ìud en tant que daemonset . Ce composant proxy impl√©mente le concept de services Kubernetes pour les pods (DNS unique pour un ensemble de pods avec √©quilibrage de charge). Il utilise principalement les proxys UDP, TCP et SCTP et ne comprend pas HTTP.

Lorsque vous exposez des pods √† l'aide d'un service (ClusterIP), Kube-proxy cr√©e des r√®gles r√©seau pour envoyer le trafic vers les pods backend (points de terminaison) regroup√©s sous l'objet Service. Autrement dit, l'√©quilibrage de charge et la d√©couverte de services sont g√©r√©s par le proxy Kube.

Vous ne pouvez pas envoyer de ping au ClusterIP car il est uniquement utilis√© pour la d√©couverte de services, contrairement aux IP de pod qui sont pingables.



# Concepts & Composants

## Namespaces

Les **namespaces** permettent d‚Äôisoler les ressources dans un cluster K8s.

- Sans namespace explicite, les ressources vont dans le `default`.
- Commande : `kubectl get all` ne montre que les ressources du namespace courant.
- Utilise `kubectl get all --all-namespaces` pour tout afficher.

## Pods

- Un **pod** est l‚Äôunit√© de base de Kubernetes.
- Contient un ou plusieurs conteneurs partageant IP, r√©seau et stockage.
- G√©r√© automatiquement : s‚Äôil crash, il est relanc√©.
- Cr√©√© par des objets comme Deployment, DaemonSet, etc.

---

## ClusterIP

ClusterIP est le type par d√©faut des Services. Il expose un Pod √† l‚Äôint√©rieur du cluster uniquement (pas √† l‚Äôext√©rieur).
Tr√®s utilis√© pour la communication interservices, m√™me si on ne le voit pas toujours directement.

## Objets principaux

| Objet         | R√¥le                                                                 |
|---------------|----------------------------------------------------------------------|
| **Pod**       | Unit√© d'ex√©cution contenant un ou plusieurs conteneurs.              |
| **Deployment**| Cr√©e et g√®re des pods r√©plicables (ex: frontend, API, workers), permet de configurer les replicaSet (on les configure tjrs les replica, jamais les replicaSet).      |
| **DaemonSet** | 1 pod par n≈ìud (ex: monitoring, agent de log).                       |
| **CronJob**   | Lance un pod √† intervalle r√©gulier (t√¢ches planifi√©es).              |
| **StatefulSet**| G√®re des pods avec identit√© stable (ex: base de donn√©es).           |
| **Service**   | Point d‚Äôacc√®s stable vers un ou plusieurs pods.                      |
| **Ingress**   | Reverse proxy pour exposer HTTP(S) avec routage par chemin/domaine.  |
| **Secret**    | Stocke des infos sensibles (mdp, tokens).                            |
| **ConfigMap** | Stocke des configs non sensibles (variables, fichiers).              |

---

## Types de Services

| Type de service  | Accessible depuis           | Description rapide                                                                 |
|------------------|-----------------------------|-------------------------------------------------------------------------------------|
| `ClusterIP`      | *Interne au cluster*        | Par d√©faut. Utilis√© pour les communications internes entre pods.                   |
| `NodePort`       | *IP de chaque n≈ìud*         | Ouvre un port statique (`<IP-node>:port`). Simplicit√© mais peu flexible.          |
| `LoadBalancer`   | *Internet (Cloud Provider)* | Cr√©e un vrai Load Balancer (cloud uniquement ou MetalLB en local).                |
| `ExternalName`   | *Redirection DNS*           | Redirige vers un DNS externe (ex: `api.example.com`).                              |
| **Ingress**      | *HTTP(S) unique*            | Reverse proxy pour exposer plusieurs services via un seul point d‚Äôentr√©e.         |

---

## ConfigMap et Secret

ConfigMap pour des donn√©es non sensibles,
Secret pour des donn√©es sensibles (chiffr√©es en base64, pas vraiment "s√©curis√©es" au repos mais mieux que rien).


## LoadBalancer

Fonctionnement (en environnement Cloud) :

1. Tu d√©clares un `Service` avec `type: LoadBalancer`.
2. Kubernetes demande √† ton cloud provider (via leur API) de cr√©er un load balancer r√©el.
3. Le provider :
   - Cr√©e le LB
   - Lui donne une IP publique
   - Route le trafic vers les nodes du cluster
4. Le service distribue le trafic vers les bons pods.

üìù *En local, utilise [MetalLB](https://metallb.universe.tf/) pour simuler ce comportement.*

---

## Ingress (et pourquoi le pr√©f√©rer)

Ingress = Objet Kubernetes
Ingress Controller = Pod qui impl√©mente cet objet (ex : nginx, traefik)

| Option        | ‚úÖ Avantages                          | ‚ùå Inconv√©nients                                     |
|---------------|--------------------------------------|-----------------------------------------------------|
| NodePort      | Facile pour tester en local          | Moins s√©curis√©, peu flexible                        |
| LoadBalancer  | Propre en cloud                      | Cher (un LB par service), inutile sans cloud        |
| Ingress       | Id√©al pour exposer plusieurs services | N√©cessite un Ingress Controller (NGINX, Traefik...) |

### Ingress = Le choix moderne

- Un seul point d‚Äôentr√©e
- Routage par chemin (`/api`, `/ws`) ou domaine (`api.monsite.com`)
- Support du HTTPS, Let‚Äôs Encrypt, cert-manager
- Scalable, maintenable, propre

---

## ConfigMap / Secret

| Ressource    | Utilit√©                             |
|--------------|--------------------------------------|
| **ConfigMap**| Param√®tres non sensibles (env, fichiers config) |
| **Secret**   | Donn√©es sensibles (tokens, mdp...)   |

---

## Stockage : Volume / PVC

- Un pod est **√©ph√©m√®re**, donc ses donn√©es disparaissent √† sa mort.
- Pour persister les donn√©es :
  - Utilise un `PersistentVolume`
  - Requis : `PersistentVolumeClaim` (PVC) pour s‚Äôy connecter
- Utilis√© avec `StatefulSet` ou pour les bases de donn√©es.


## Probes

- **Liveness Probe** : red√©marre le conteneur s‚Äôil ne r√©pond plus.
- **Readiness Probe** : signale si le conteneur est pr√™t √† recevoir du trafic.
- Important pour la sant√© et la stabilit√© des services.

## Les Quorums

K8s utilise etcd comme base de donn√©es centrale.
Etcd fonctionne √† quorum :

Si tu as 3 n≈ìuds etcd ‚Üí quorum = 2

Si tu as 5 n≈ìuds etcd ‚Üí quorum = 3

Et ainsi de suite : quorum = (N / 2) + 1

‚û°Ô∏è Si le quorum n‚Äôest pas atteint, plus aucune √©criture ne peut avoir lieu.

| Cas                       | Quorum utile pour...                                       |
| ------------------------- | ---------------------------------------------------------- |
| Kubernetes (etcd)         | √âviter corruption de l‚Äô√©tat du cluster                     |
| Kafka                     | G√©rer qui est le leader, assurer la coh√©rence des messages |
| Base de donn√©es r√©pliqu√©e | Assurer que la majorit√© a bien re√ßu les √©critures          |
| √âlection de leader        | D√©cider qui prend le contr√¥le sans conflit                 |

## RBAC Authorization

L'autorisation RBAC utilise lerbac.authorization.k8s.io Groupe APIpour piloter les d√©cisions d'autorisation, vous permettant de configurer dynamiquement les politiques via l'API Kubernetes.

Pour activer RBAC, d√©marrez leserveur API avec l' --authorization-configindicateur d√©fini sur un fichier qui inclut l' RBACautorisateur ; par exemple :

manifest example: 

  apiVersion: apiserver.config.k8s.io/v1
  kind: AuthorizationConfiguration
  authorizers:
    ...
    - type: RBAC
    ...

L'API RBAC d√©clare quatre types d'objets Kubernetes : Role , ClusterRole , RoleBinding et ClusterRoleBinding . Vous pouvez d√©crire ou modifier le RBAC. objets en utilisant des outils tels que kubectl, comme tout autre objet Kubernetes.

![Voir doc pour en apprendre plus sur: Role, ClusterRole, RoleBinding et ClusterRoleBinding](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)

## Other orchestrator 

### Nomad 

[guide](https://blog.cloudflare.com/how-we-use-hashicorp-nomad/?ref=devopscube.com/)
[Illustration](/thotify/k8s_&_devops/image3.png)


## Tools

## For Dev Environment

1. Vagrant  
2. Docker Desktop  
3. Minikube (k8s)  
4. Minishift (k8s)  
5. Kind (k8s)  

## For Infrastructure Provisioning

1. Terraform (preferable)  
2. CloudFormation for AWS  
3. CLIs (of respective cloud provider)  
4. Pulumi  

## For Configuration Management

1. Ansible (preferable)  
2. Chef  
3. Puppet  
4. Saltstack  

## VM Image / Container Management

1. HashiCorp Packer  
2. Docker  


### Helm

Helm est un gestionnaire de configuration et de paquets pour Kubernetes.
Il permet de d√©ployer facilement des applications complexes sur un cluster Kubernetes √† l‚Äôaide de Helm Charts (graphes de d√©ploiement).

Il offre de puissantes fonctionnalit√©s de templating, prenant en charge la g√©n√©ration de templates pour tous les objets Kubernetes :
Deployments, Pods, Services, ConfigMaps, Secrets, RBAC, PSP, etc.

### Kustomize 
Kustomize repose sur deux concepts cl√©s : Base et Overlays.

Avec Kustomize, on peut r√©utiliser des fichiers de base (fichiers YAML communs) pour tous les environnements, et y appliquer des overlays (patchs) sp√©cifiques √† chaque environnement.

Le processus de overlaying consiste √† cr√©er une version personnalis√©e d‚Äôun manifest, en combinant :

manifest de base + manifest overlay = manifest personnalis√©

Toutes les sp√©cifications de personnalisation sont d√©finies dans un fichier appel√© kustomization.yaml.
[Illustration](/thotify/k8s_&_devops/image-29-22.png)

 























# Ressources
![Comprendre K8s dans le d√©tails, la plupart des liens ci-dessous sont tir√©s de cet article](https://devopscube.com/kubernetes-architecture-explained/)
![Comprendre la concurrence RAFT](https://raft.github.io/) 
![Th√©or√®me CAP & concurrence ](https://en.wikipedia.org/wiki/CAP_theorem?ref=devopscube.com)
![Gestion de la planification avanc√©e](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md)